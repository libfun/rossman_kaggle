{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "import xgboost as xgb\n",
    "\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_nnew.csv', low_memory=False)\n",
    "test = pd.read_csv('test_nnew.csv', low_memory=False)\n",
    "features = [u'Open', u'Promo', u'SchoolHoliday', u'StateHoliday_0',\n",
    "       u'StateHoliday_a', u'DayOfWeek_1', u'DayOfWeek_2', u'DayOfWeek_3',\n",
    "       u'DayOfWeek_4', u'DayOfWeek_5', u'DayOfWeek_6', u'DayOfWeek_7',\n",
    "       u'CompetitionDistance', u'Promo2', 'year', 'Mean_Sales', 'month', 'day',\n",
    "       u'StoreType_a', u'StoreType_b', u'StoreType_c', u'StoreType_d',\n",
    "       u'Assortment_a', u'Assortment_b', u'Assortment_c', u'CompetitionOpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['year'] = train.Date.apply(lambda x: x.split('-')[0])\n",
    "train['year'] = train['year'].astype(float)\n",
    "train['month'] = train.Date.apply(lambda x: x.split('-')[1])\n",
    "train['month'] = train['month'].astype(float)\n",
    "train['day'] = train.Date.apply(lambda x: x.split('-')[2])\n",
    "train['day'] = train['day'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['year'] = test.Date.apply(lambda x: x.split('-')[0])\n",
    "test['year'] = test['year'].astype(float)\n",
    "test['month'] = test.Date.apply(lambda x: x.split('-')[1])\n",
    "test['month'] = test['month'].astype(float)\n",
    "test['day'] = test.Date.apply(lambda x: x.split('-')[2])\n",
    "test['day'] = test['day'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns = [u'Store', u'DayOfWeek', u'Date', u'Sales', u'Customers', u'Open',\n",
    "       u'Promo', u'StateHoliday', u'SchoolHoliday', u'StateHoliday0',\n",
    "       u'StateHolidaya', u'DayOfWeek1', u'DayOfWeek2', u'DayOfWeek3',\n",
    "       u'DayOfWeek4', u'DayOfWeek5', u'DayOfWeek6', u'DayOfWeek7',\n",
    "       u'CompetitionDistance', u'Promo2', u'CompetitionOpenSince',\n",
    "       u'StoreTypea', u'StoreTypeb', u'StoreTypec', u'StoreTyped',\n",
    "       u'Assortmenta', u'Assortmentb', u'Assortmentc', u'CompetitionOpen',\n",
    "       u'MeanSales', u'year', u'month', u'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.columns = [u'Id', u'Store', u'DayOfWeek', u'Date', u'Open', u'Promo',\n",
    "       u'StateHoliday', u'SchoolHoliday', u'StateHolidaya', u'StateHoliday0',\n",
    "       u'DayOfWeek1', u'DayOfWeek2', u'DayOfWeek3', u'DayOfWeek4',\n",
    "       u'DayOfWeek5', u'DayOfWeek6', u'DayOfWeek7', u'CompetitionDistance',\n",
    "       u'Promo2', u'CompetitionOpenSince', u'StoreTypea', u'StoreTypeb',\n",
    "       u'StoreTypec', u'StoreTyped', u'Assortmenta', u'Assortmentb',\n",
    "       u'Assortmentc', u'CompetitionOpen', u'MeanSales', u'year', u'month',\n",
    "       u'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [u'Open', u'Promo', u'StateHoliday0',\n",
    "       u'StateHolidaya', u'DayOfWeek1', u'DayOfWeek2', u'DayOfWeek3',\n",
    "       u'DayOfWeek4', u'DayOfWeek5', u'DayOfWeek6', u'DayOfWeek7',\n",
    "       u'CompetitionDistance', u'Promo2', 'year', 'MeanSales', 'month', 'day',\n",
    "       u'StoreTypea', u'StoreTypeb', u'StoreTypec', u'StoreTyped',\n",
    "       u'Assortmenta', u'Assortmentb', u'Assortmentc', u'CompetitionOpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_features = [u'Open', u'Promo', u'StateHoliday0',\n",
    "       u'StateHolidaya', u'DayOfWeek1', u'DayOfWeek2', u'DayOfWeek3',\n",
    "       u'DayOfWeek4', u'DayOfWeek5', u'DayOfWeek6', u'DayOfWeek7',\n",
    "       u'CompetitionDistance', u'Promo2', 'year', 'MeanSales', 'month', 'day', u'CompetitionOpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['Date'] = train['Date'].astype('datetime64')\n",
    "test['Date'] = test['Date'].astype('datetime64')\n",
    "train['CompetitionOpenSince'] = train['CompetitionOpenSince'].astype('datetime64')\n",
    "test['CompetitionOpenSince'] = test['CompetitionOpenSince'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train[train['Store'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.sort(['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.index = np.arange(len(train_x.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stores = np.unique(test.Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_nosh = [               u'Store',            u'DayOfWeek',              u'Sales',            u'Open',\n",
    "                      u'Promo',         \n",
    "              u'SchoolHoliday',        u'StateHoliday0',\n",
    "              u'StateHolidaya',           u'DayOfWeek1',\n",
    "                 u'DayOfWeek2',           u'DayOfWeek3',\n",
    "                 u'DayOfWeek4',           u'DayOfWeek5',\n",
    "                 u'DayOfWeek6',           u'DayOfWeek7',\n",
    "        u'CompetitionDistance',               u'Promo2',           u'StoreTypea',\n",
    "                 u'StoreTypeb',           u'StoreTypec',\n",
    "                 u'StoreTyped',          u'Assortmenta',\n",
    "                u'Assortmentb',          u'Assortmentc',\n",
    "            u'CompetitionOpen',            u'MeanSales',\n",
    "                       u'year',                u'month',\n",
    "                        u'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allfs = features + [i+'mean' for i in ts_features] + [i+'std' for i in ts_features] + [i+'dta' for i in ts_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_7.loc[6,[i+'dta' for i in cols_nosh]] = (train_x.loc[max(6-1, 0),cols_nosh] - train_x.loc[max(6-6, 0),cols_nosh]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  from ipykernel import kernelapp as app\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "train_x = train[train['Store'] == store]\n",
    "train_x = train_x.sort(['Date'])\n",
    "train_x.index = np.arange(len(train_x.index))\n",
    "test_x = test[test['Store'] == store]\n",
    "test_x = test_x.sort(['Date'])\n",
    "test_x.index = np.arange(len(test_x.index))\n",
    "train_7 = train_x.copy()\n",
    "train_18 = train_x.copy()\n",
    "train_30 = train_x.copy()\n",
    "test_7 = test_x.copy()\n",
    "test_18 = test_x.copy()\n",
    "test_30 = test_x.copy()\n",
    "for f in [i+'mean' for i in ts_features] + [i+'std' for i in ts_features] + [i+'dta' for i in ts_features]:\n",
    "    train_7.loc[:,f] = range(len(train_7))\n",
    "    test_7.loc[:,f] = range(len(test_7))\n",
    "for step in xrange(6, len(train_7)):\n",
    "    s = max(step-6,0)\n",
    "    st = max(step-1, 0)\n",
    "    train_7.loc[step,[i+'mean' for i in ts_features]] = (train_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "    train_7.loc[step,[i+'std' for i in ts_features]] = (train_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "    train_7.loc[step,[i+'dta' for i in ts_features]] = (train_x.loc[st,ts_features] - \\\n",
    "                                                        train_x.loc[s,ts_features]).values\n",
    "    #train_7.loc[step-6,'Sales'] = train_x.loc[step+1,'Sales']\n",
    "    #train_7.loc[step-6,'Open'] = train_x.loc[step+1,'Open']\n",
    "train_7 = train_7.loc[6:,:]\n",
    "for step in xrange(len(test_7)):\n",
    "    s = max(step-6,0)\n",
    "    st = max(step-1, 0)\n",
    "    test_7.loc[step,[i+'mean' for i in ts_features]] = (test_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "    test_7.loc[step,[i+'std' for i in ts_features]] = (test_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "    test_7.loc[step,[i+'dta' for i in ts_features]] = (test_x.loc[st,ts_features] - \\\n",
    "                                                       train_x.loc[s,ts_features]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_7 = test_7.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "space = (hp.quniform('eta', 0.01, 1),\n",
    "         hp.quniform('subsample', 0.01, 1),\n",
    "         hp.quniform('colsample_bytree', 0.01, 1),\n",
    "         hp.quniform('lambda', 0, 1000),\n",
    "         hp.quniform('alpha', 0, 1),\n",
    "         hp.randint('max_depth', 1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.05, random_state = 1)\n",
    "dtrain = xgb.DMatrix(X_train[X_train['Open'] > 0][features], np.log(X_train[X_train['Open'] > 0][\"Sales\"] + 1))\n",
    "dvalid = xgb.DMatrix(X_test[X_test['Open'] > 0][features], np.log(X_test[X_test['Open'] > 0][\"Sales\"] + 1))\n",
    "\n",
    "def calc(params):\n",
    "\n",
    "    print params\n",
    "    \n",
    "    num_trees = 5000\n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, \n",
    "                    feval=rmspe_xg, verbose_eval=False)\n",
    "\n",
    "    #print(\"Validating\")\n",
    "    train_probs = gbm.predict(dvalid)\n",
    "    indices = train_probs < 0\n",
    "    train_probs[indices] = 0\n",
    "    error = rmspe(np.exp(train_probs) - 1, X_test[X_test['Open'] > 0]['Sales'].values)\n",
    "    print 'Error:', error\n",
    "    return {'loss': error, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def score(params):\n",
    "    print \"Training with params : \"\n",
    "    print params\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid).reshape((X_test.shape[0], 9))\n",
    "    score = log_loss(y_test, predictions)\n",
    "    print \"\\tScore {0}\\n\\n\".format(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize():\n",
    "    space = {\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 20, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.2, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.2, 1, 0.05),\n",
    "             'alpha' : hp.quniform('alpha', 0, 1, 0.05),\n",
    "             'lambda' : hp.quniform('lambda', 0, 1000, 1),\n",
    "             'objective': 'reg:linear',\n",
    "             'silent' : 1\n",
    "             }\n",
    "\n",
    "    best = fmin(calc, space, algo=tpe.suggest, max_evals=2500)\n",
    "\n",
    "    print best\n",
    "\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "#trials = Trials()\n",
    "\n",
    "optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"reg:linear\", \n",
    "          \"eta\": 0.5, \n",
    "          \"max_depth\": 8, \n",
    "          \"subsample\": 0.7, \n",
    "          \"colsample_bytree\": 0.7, \n",
    "          \"silent\": 1, \n",
    "          #\"lambda\" : 100, \n",
    "          #\"alpha\" : 1\n",
    "         } \n",
    "num_trees = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until train error hasn't decreased in 50 rounds.\n",
      "[0]\teval-rmspe:0.981450\ttrain-rmspe:0.981210\n",
      "[1]\teval-rmspe:0.863841\ttrain-rmspe:0.861923\n",
      "[2]\teval-rmspe:0.635097\ttrain-rmspe:0.631027\n",
      "[3]\teval-rmspe:0.412703\ttrain-rmspe:0.400980\n",
      "[4]\teval-rmspe:0.268685\ttrain-rmspe:0.241932\n",
      "[5]\teval-rmspe:0.183928\ttrain-rmspe:0.153977\n",
      "[6]\teval-rmspe:0.158886\ttrain-rmspe:0.110336\n",
      "[7]\teval-rmspe:0.138767\ttrain-rmspe:0.088372\n",
      "[8]\teval-rmspe:0.134259\ttrain-rmspe:0.074500\n",
      "[9]\teval-rmspe:0.128839\ttrain-rmspe:0.068683\n",
      "[10]\teval-rmspe:0.128966\ttrain-rmspe:0.062903\n",
      "[11]\teval-rmspe:0.133170\ttrain-rmspe:0.055506\n",
      "[12]\teval-rmspe:0.133585\ttrain-rmspe:0.051931\n",
      "[13]\teval-rmspe:0.134014\ttrain-rmspe:0.048617\n",
      "[14]\teval-rmspe:0.132504\ttrain-rmspe:0.046849\n",
      "[15]\teval-rmspe:0.135449\ttrain-rmspe:0.043702\n",
      "[16]\teval-rmspe:0.136765\ttrain-rmspe:0.041930\n",
      "[17]\teval-rmspe:0.140413\ttrain-rmspe:0.040519\n",
      "[18]\teval-rmspe:0.135486\ttrain-rmspe:0.037454\n",
      "[19]\teval-rmspe:0.133426\ttrain-rmspe:0.034702\n",
      "[20]\teval-rmspe:0.133223\ttrain-rmspe:0.031302\n",
      "[21]\teval-rmspe:0.134951\ttrain-rmspe:0.028485\n",
      "[22]\teval-rmspe:0.136163\ttrain-rmspe:0.026164\n",
      "[23]\teval-rmspe:0.137243\ttrain-rmspe:0.025254\n",
      "[24]\teval-rmspe:0.136288\ttrain-rmspe:0.023577\n",
      "[25]\teval-rmspe:0.135969\ttrain-rmspe:0.021881\n",
      "[26]\teval-rmspe:0.134229\ttrain-rmspe:0.019932\n",
      "[27]\teval-rmspe:0.135005\ttrain-rmspe:0.018086\n",
      "[28]\teval-rmspe:0.136184\ttrain-rmspe:0.016323\n",
      "[29]\teval-rmspe:0.136176\ttrain-rmspe:0.016062\n",
      "[30]\teval-rmspe:0.136261\ttrain-rmspe:0.014420\n",
      "[31]\teval-rmspe:0.135963\ttrain-rmspe:0.013618\n",
      "[32]\teval-rmspe:0.134461\ttrain-rmspe:0.012788\n",
      "[33]\teval-rmspe:0.134122\ttrain-rmspe:0.011595\n",
      "[34]\teval-rmspe:0.135129\ttrain-rmspe:0.010922\n",
      "[35]\teval-rmspe:0.134948\ttrain-rmspe:0.010285\n",
      "[36]\teval-rmspe:0.134226\ttrain-rmspe:0.009320\n",
      "[37]\teval-rmspe:0.133226\ttrain-rmspe:0.008524\n",
      "[38]\teval-rmspe:0.133216\ttrain-rmspe:0.008153\n",
      "[39]\teval-rmspe:0.132713\ttrain-rmspe:0.007601\n",
      "[40]\teval-rmspe:0.133718\ttrain-rmspe:0.007159\n",
      "[41]\teval-rmspe:0.133952\ttrain-rmspe:0.006702\n",
      "[42]\teval-rmspe:0.134174\ttrain-rmspe:0.006395\n",
      "[43]\teval-rmspe:0.134072\ttrain-rmspe:0.005887\n",
      "[44]\teval-rmspe:0.134102\ttrain-rmspe:0.005684\n",
      "[45]\teval-rmspe:0.133599\ttrain-rmspe:0.005341\n",
      "[46]\teval-rmspe:0.133552\ttrain-rmspe:0.004948\n",
      "[47]\teval-rmspe:0.133539\ttrain-rmspe:0.004559\n",
      "[48]\teval-rmspe:0.133274\ttrain-rmspe:0.004208\n",
      "[49]\teval-rmspe:0.133508\ttrain-rmspe:0.004050\n",
      "[50]\teval-rmspe:0.133621\ttrain-rmspe:0.003772\n",
      "[51]\teval-rmspe:0.133626\ttrain-rmspe:0.003604\n",
      "[52]\teval-rmspe:0.133759\ttrain-rmspe:0.003510\n",
      "[53]\teval-rmspe:0.133791\ttrain-rmspe:0.003344\n",
      "[54]\teval-rmspe:0.133791\ttrain-rmspe:0.003241\n",
      "[55]\teval-rmspe:0.133760\ttrain-rmspe:0.003196\n",
      "[56]\teval-rmspe:0.134003\ttrain-rmspe:0.003037\n",
      "[57]\teval-rmspe:0.134041\ttrain-rmspe:0.002896\n",
      "[58]\teval-rmspe:0.134091\ttrain-rmspe:0.002724\n",
      "[59]\teval-rmspe:0.134135\ttrain-rmspe:0.002701\n",
      "[60]\teval-rmspe:0.134190\ttrain-rmspe:0.002677\n",
      "[61]\teval-rmspe:0.134187\ttrain-rmspe:0.002651\n",
      "[62]\teval-rmspe:0.134189\ttrain-rmspe:0.002650\n",
      "[63]\teval-rmspe:0.134147\ttrain-rmspe:0.002571\n",
      "[64]\teval-rmspe:0.134132\ttrain-rmspe:0.002530\n",
      "[65]\teval-rmspe:0.134118\ttrain-rmspe:0.002523\n",
      "[66]\teval-rmspe:0.134118\ttrain-rmspe:0.002523\n",
      "[67]\teval-rmspe:0.133968\ttrain-rmspe:0.002491\n",
      "[68]\teval-rmspe:0.133967\ttrain-rmspe:0.002481\n",
      "[69]\teval-rmspe:0.134015\ttrain-rmspe:0.002445\n",
      "[70]\teval-rmspe:0.133981\ttrain-rmspe:0.002368\n",
      "[71]\teval-rmspe:0.133935\ttrain-rmspe:0.002323\n",
      "[72]\teval-rmspe:0.133936\ttrain-rmspe:0.002310\n",
      "[73]\teval-rmspe:0.133957\ttrain-rmspe:0.002309\n",
      "[74]\teval-rmspe:0.133954\ttrain-rmspe:0.002309\n",
      "[75]\teval-rmspe:0.133946\ttrain-rmspe:0.002304\n",
      "[76]\teval-rmspe:0.133959\ttrain-rmspe:0.002306\n",
      "[77]\teval-rmspe:0.133916\ttrain-rmspe:0.002304\n",
      "[78]\teval-rmspe:0.133972\ttrain-rmspe:0.002302\n",
      "[79]\teval-rmspe:0.133991\ttrain-rmspe:0.002307\n",
      "[80]\teval-rmspe:0.134050\ttrain-rmspe:0.002264\n",
      "[81]\teval-rmspe:0.133959\ttrain-rmspe:0.002219\n",
      "[82]\teval-rmspe:0.133971\ttrain-rmspe:0.002218\n",
      "[83]\teval-rmspe:0.133963\ttrain-rmspe:0.002219\n",
      "[84]\teval-rmspe:0.133967\ttrain-rmspe:0.002210\n",
      "[85]\teval-rmspe:0.133961\ttrain-rmspe:0.002210\n",
      "[86]\teval-rmspe:0.133979\ttrain-rmspe:0.002210\n",
      "[87]\teval-rmspe:0.134020\ttrain-rmspe:0.002194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "('error', 0.13416380876664089)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[88]\teval-rmspe:0.133993\ttrain-rmspe:0.002180\n",
      "[89]\teval-rmspe:0.133997\ttrain-rmspe:0.002180\n",
      "[90]\teval-rmspe:0.133994\ttrain-rmspe:0.002180\n",
      "[91]\teval-rmspe:0.134006\ttrain-rmspe:0.002173\n",
      "[92]\teval-rmspe:0.134009\ttrain-rmspe:0.002154\n",
      "[93]\teval-rmspe:0.134011\ttrain-rmspe:0.002154\n",
      "[94]\teval-rmspe:0.134015\ttrain-rmspe:0.002155\n",
      "[95]\teval-rmspe:0.134027\ttrain-rmspe:0.002139\n",
      "[96]\teval-rmspe:0.134014\ttrain-rmspe:0.002135\n",
      "[97]\teval-rmspe:0.134022\ttrain-rmspe:0.002136\n",
      "[98]\teval-rmspe:0.134150\ttrain-rmspe:0.002129\n",
      "[99]\teval-rmspe:0.134164\ttrain-rmspe:0.002129\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = cross_validation.train_test_split(train_7, test_size=0.05, random_state = 1)\n",
    "dtrain = xgb.DMatrix(X_train[X_train['Open'] > 0][allfs], np.log(X_train[X_train['Open'] > 0][\"Sales\"] + 1))\n",
    "dvalid = xgb.DMatrix(X_test[X_test['Open'] > 0][allfs], np.log(X_test[X_test['Open'] > 0][\"Sales\"] + 1))\n",
    "dtest = xgb.DMatrix(test_7[allfs])\n",
    "watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, \n",
    "                feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "print(\"Validating\")\n",
    "train_probs = gbm.predict(dvalid)\n",
    "indices = train_probs < 0\n",
    "train_probs[indices] = 0\n",
    "error = rmspe(np.exp(train_probs) - 1, X_test[X_test['Open'] > 0]['Sales'].values)\n",
    "print('error', error)\n",
    "\n",
    "#test_probs_18 = gbm.predict(dtest)\n",
    "#indices = test_probs_18 < 0\n",
    "#test_probs_18[indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = test[['Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_a7 = pd.DataFrame(columns=allfs)\n",
    "train_a18 = pd.DataFrame(columns=allfs)\n",
    "train_a30 = pd.DataFrame(columns=allfs)\n",
    "test_a7 = pd.DataFrame(columns=allfs + ['Id'])\n",
    "test_a18 = pd.DataFrame(columns=allfs + ['Id'])\n",
    "test_a30 = pd.DataFrame(columns=allfs + ['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store num 1\n",
      "store num 3\n",
      "store num 7\n",
      "store num 8\n",
      "store num 9\n",
      "store num 10\n",
      "store num 11\n",
      "store num 12\n",
      "store num 13\n",
      "store num 14\n",
      "store num 15\n",
      "store num 16\n",
      "store num 19\n",
      "store num 20\n",
      "store num 21\n",
      "store num 22\n",
      "store num 23\n",
      "store num 24\n",
      "store num 25\n",
      "store num 27\n",
      "store num 29\n",
      "store num 30\n",
      "store num 31\n",
      "store num 32\n",
      "store num 33\n",
      "store num 35\n",
      "store num 36\n",
      "store num 38\n",
      "store num 39\n",
      "store num 40\n",
      "store num 41\n",
      "store num 42\n",
      "store num 43\n",
      "store num 45\n",
      "store num 46\n",
      "store num 47\n",
      "store num 48\n",
      "store num 49\n",
      "store num 50\n",
      "store num 51\n",
      "store num 52\n",
      "store num 53\n",
      "store num 56\n",
      "store num 58\n",
      "store num 61\n",
      "store num 62\n",
      "store num 63\n",
      "store num 64\n",
      "store num 66\n",
      "store num 67\n",
      "store num 68\n",
      "store num 69\n",
      "store num 70\n",
      "store num 71\n",
      "store num 72\n",
      "store num 73\n",
      "store num 74\n",
      "store num 75\n",
      "store num 76\n",
      "store num 77\n",
      "store num 79\n",
      "store num 80\n",
      "store num 81\n",
      "store num 82\n",
      "store num 83\n",
      "store num 84\n",
      "store num 86\n",
      "store num 89\n",
      "store num 90\n",
      "store num 91\n",
      "store num 92\n",
      "store num 93\n",
      "store num 94\n",
      "store num 98\n",
      "store num 99\n",
      "store num 100\n",
      "store num 101\n",
      "store num 102\n",
      "store num 105\n",
      "store num 107\n",
      "store num 108\n",
      "store num 109\n",
      "store num 110\n",
      "store num 111\n",
      "store num 112\n",
      "store num 113\n",
      "store num 115\n",
      "store num 117\n",
      "store num 118\n",
      "store num 119\n",
      "store num 120\n",
      "store num 122\n",
      "store num 124\n",
      "store num 126\n",
      "store num 127\n",
      "store num 128\n",
      "store num 129\n",
      "store num 130\n",
      "store num 131\n",
      "store num 132\n",
      "store num 135\n",
      "store num 136\n",
      "store num 137\n",
      "store num 139\n",
      "store num 140\n",
      "store num 141\n",
      "store num 142\n",
      "store num 143\n",
      "store num 144\n",
      "store num 145\n",
      "store num 146\n",
      "store num 147\n",
      "store num 149\n",
      "store num 150\n",
      "store num 152\n",
      "store num 153\n",
      "store num 154\n",
      "store num 155\n",
      "store num 157\n",
      "store num 158\n",
      "store num 159\n",
      "store num 160\n",
      "store num 161\n",
      "store num 162\n",
      "store num 164\n",
      "store num 165\n",
      "store num 166\n",
      "store num 168\n",
      "store num 169\n",
      "store num 170\n",
      "store num 171\n",
      "store num 172\n",
      "store num 174\n",
      "store num 175\n",
      "store num 176\n",
      "store num 179\n",
      "store num 180\n",
      "store num 181\n",
      "store num 182\n",
      "store num 183\n",
      "store num 184\n",
      "store num 186\n",
      "store num 187\n",
      "store num 188\n",
      "store num 189\n",
      "store num 190\n",
      "store num 191\n",
      "store num 192\n",
      "store num 193\n",
      "store num 194\n",
      "store num 195\n",
      "store num 197\n",
      "store num 199\n",
      "store num 200\n",
      "store num 201\n",
      "store num 202\n",
      "store num 204\n",
      "store num 206\n",
      "store num 207\n",
      "store num 209\n",
      "store num 210\n",
      "store num 212\n",
      "store num 213\n",
      "store num 214\n",
      "store num 215\n",
      "store num 216\n",
      "store num 217\n",
      "store num 218\n",
      "store num 219\n",
      "store num 220\n",
      "store num 221\n",
      "store num 224\n",
      "store num 226\n",
      "store num 227\n",
      "store num 228\n",
      "store num 229\n",
      "store num 230\n",
      "store num 231\n",
      "store num 233\n",
      "store num 234\n",
      "store num 235\n",
      "store num 238\n",
      "store num 239\n",
      "store num 241\n",
      "store num 242\n",
      "store num 243\n",
      "store num 244\n",
      "store num 245\n",
      "store num 246\n",
      "store num 247\n",
      "store num 248\n",
      "store num 249\n",
      "store num 250\n",
      "store num 251\n",
      "store num 252\n",
      "store num 253\n",
      "store num 254\n",
      "store num 255\n",
      "store num 256\n",
      "store num 258\n",
      "store num 259\n",
      "store num 262\n",
      "store num 263\n",
      "store num 265\n",
      "store num 267\n",
      "store num 268\n",
      "store num 269\n",
      "store num 272\n",
      "store num 273\n",
      "store num 274\n",
      "store num 275\n",
      "store num 277\n",
      "store num 278\n",
      "store num 279\n",
      "store num 280\n",
      "store num 281\n",
      "store num 283\n",
      "store num 284\n",
      "store num 285\n",
      "store num 287\n",
      "store num 288\n",
      "store num 289\n",
      "store num 290\n",
      "store num 294\n",
      "store num 295\n",
      "store num 296\n",
      "store num 297\n",
      "store num 298\n",
      "store num 299\n",
      "store num 300\n",
      "store num 301\n",
      "store num 302\n",
      "store num 303\n",
      "store num 304\n",
      "store num 305\n",
      "store num 306\n",
      "store num 308\n",
      "store num 309\n",
      "store num 310\n",
      "store num 311\n",
      "store num 312\n",
      "store num 314\n",
      "store num 315\n",
      "store num 316\n",
      "store num 317\n",
      "store num 319\n",
      "store num 320\n",
      "store num 323\n",
      "store num 325\n",
      "store num 326\n",
      "store num 328\n",
      "store num 329\n",
      "store num 330\n",
      "store num 331\n",
      "store num 333\n",
      "store num 334\n",
      "store num 335\n",
      "store num 336\n",
      "store num 337\n",
      "store num 338\n",
      "store num 339\n",
      "store num 340\n",
      "store num 341\n",
      "store num 342\n",
      "store num 343\n",
      "store num 344\n",
      "store num 345\n",
      "store num 346\n",
      "store num 347\n",
      "store num 348\n",
      "store num 350\n",
      "store num 351\n",
      "store num 352\n",
      "store num 353\n",
      "store num 354\n",
      "store num 355\n",
      "store num 356\n",
      "store num 358\n",
      "store num 359\n",
      "store num 362\n",
      "store num 364\n",
      "store num 365\n",
      "store num 367\n",
      "store num 368\n",
      "store num 369\n",
      "store num 370\n",
      "store num 371\n",
      "store num 372\n",
      "store num 373\n",
      "store num 377\n",
      "store num 378\n",
      "store num 379\n",
      "store num 380\n",
      "store num 383\n",
      "store num 385\n",
      "store num 386\n",
      "store num 387\n",
      "store num 388\n",
      "store num 389\n",
      "store num 391\n",
      "store num 392\n",
      "store num 393\n",
      "store num 394\n",
      "store num 395\n",
      "store num 397\n",
      "store num 398\n",
      "store num 399\n",
      "store num 403\n",
      "store num 404\n",
      "store num 405\n",
      "store num 406\n",
      "store num 407\n",
      "store num 408\n",
      "store num 409\n",
      "store num 410\n",
      "store num 411\n",
      "store num 412\n",
      "store num 413\n",
      "store num 414\n",
      "store num 415\n",
      "store num 416\n",
      "store num 417\n",
      "store num 418\n",
      "store num 420\n",
      "store num 421\n",
      "store num 422\n",
      "store num 424\n",
      "store num 425\n",
      "store num 426\n",
      "store num 427\n",
      "store num 428\n",
      "store num 429\n",
      "store num 430\n",
      "store num 431\n",
      "store num 432\n",
      "store num 433\n",
      "store num 434\n",
      "store num 435\n",
      "store num 440\n",
      "store num 441\n",
      "store num 442\n",
      "store num 445\n",
      "store num 446\n",
      "store num 447\n",
      "store num 448\n",
      "store num 449\n",
      "store num 450\n",
      "store num 451\n",
      "store num 452\n",
      "store num 453\n",
      "store num 455\n",
      "store num 456\n",
      "store num 457\n",
      "store num 458\n",
      "store num 459\n",
      "store num 461\n",
      "store num 463\n",
      "store num 465\n",
      "store num 466\n",
      "store num 467\n",
      "store num 468\n",
      "store num 470\n",
      "store num 471\n",
      "store num 472\n",
      "store num 473\n",
      "store num 475\n",
      "store num 477\n",
      "store num 481\n",
      "store num 484\n",
      "store num 485\n",
      "store num 486\n",
      "store num 487\n",
      "store num 488\n",
      "store num 490\n",
      "store num 491\n",
      "store num 492\n",
      "store num 493\n",
      "store num 495\n",
      "store num 497\n",
      "store num 498\n",
      "store num 499\n",
      "store num 500\n",
      "store num 501\n",
      "store num 502\n",
      "store num 504\n",
      "store num 505\n",
      "store num 506\n",
      "store num 507\n",
      "store num 508\n",
      "store num 509\n",
      "store num 510\n",
      "store num 511\n",
      "store num 512\n",
      "store num 514\n",
      "store num 515\n",
      "store num 516\n",
      "store num 517\n",
      "store num 518\n",
      "store num 519\n",
      "store num 520\n",
      "store num 521\n",
      "store num 522\n",
      "store num 524\n",
      "store num 527\n",
      "store num 528\n",
      "store num 529\n",
      "store num 530\n",
      "store num 531\n",
      "store num 532\n",
      "store num 533\n",
      "store num 534\n",
      "store num 535\n",
      "store num 536\n",
      "store num 537\n",
      "store num 538\n",
      "store num 539\n",
      "store num 540\n",
      "store num 541\n",
      "store num 542\n",
      "store num 543\n",
      "store num 545\n",
      "store num 547\n",
      "store num 548\n",
      "store num 549\n",
      "store num 550\n",
      "store num 551\n",
      "store num 552\n",
      "store num 553\n",
      "store num 554\n",
      "store num 555\n",
      "store num 557\n",
      "store num 558\n",
      "store num 561\n",
      "store num 562\n",
      "store num 563\n",
      "store num 564\n",
      "store num 565\n",
      "store num 566\n",
      "store num 567\n",
      "store num 568\n",
      "store num 570\n",
      "store num 571\n",
      "store num 572\n",
      "store num 573\n",
      "store num 574\n",
      "store num 575\n",
      "store num 577\n",
      "store num 578\n",
      "store num 579\n",
      "store num 580\n",
      "store num 581\n",
      "store num 582\n",
      "store num 584\n",
      "store num 585\n",
      "store num 586\n",
      "store num 587\n",
      "store num 588\n",
      "store num 589\n",
      "store num 590\n",
      "store num 591\n",
      "store num 592\n",
      "store num 593\n",
      "store num 597\n",
      "store num 598\n",
      "store num 600\n",
      "store num 601\n",
      "store num 602\n",
      "store num 603\n",
      "store num 604\n",
      "store num 605\n",
      "store num 610\n",
      "store num 611\n",
      "store num 612\n",
      "store num 615\n",
      "store num 616\n",
      "store num 618\n",
      "store num 619\n",
      "store num 620\n",
      "store num 621\n",
      "store num 622\n",
      "store num 623\n",
      "store num 624\n",
      "store num 625\n",
      "store num 627\n",
      "store num 628\n",
      "store num 629\n",
      "store num 631\n",
      "store num 632\n",
      "store num 633\n",
      "store num 636\n",
      "store num 637\n",
      "store num 638\n",
      "store num 639\n",
      "store num 640\n",
      "store num 641\n",
      "store num 642\n",
      "store num 643\n",
      "store num 644\n",
      "store num 645\n",
      "store num 646\n",
      "store num 647\n",
      "store num 650\n",
      "store num 651\n",
      "store num 653\n",
      "store num 655\n",
      "store num 656\n",
      "store num 657\n",
      "store num 658\n",
      "store num 659\n",
      "store num 660\n",
      "store num 661\n",
      "store num 662\n",
      "store num 663\n",
      "store num 665\n",
      "store num 666\n",
      "store num 667\n",
      "store num 669\n",
      "store num 670\n",
      "store num 671\n",
      "store num 673\n",
      "store num 674\n",
      "store num 675\n",
      "store num 676\n",
      "store num 677\n",
      "store num 678\n",
      "store num 680\n",
      "store num 681\n",
      "store num 684\n",
      "store num 685\n",
      "store num 687\n",
      "store num 689\n",
      "store num 690\n",
      "store num 691\n",
      "store num 692\n",
      "store num 693\n",
      "store num 694\n",
      "store num 695\n",
      "store num 696\n",
      "store num 697\n",
      "store num 699\n",
      "store num 700\n",
      "store num 701\n",
      "store num 702\n",
      "store num 703\n",
      "store num 705\n",
      "store num 706\n",
      "store num 707\n",
      "store num 710\n",
      "store num 711\n",
      "store num 712\n",
      "store num 713\n",
      "store num 714\n",
      "store num 716\n",
      "store num 717\n",
      "store num 718\n",
      "store num 719\n",
      "store num 720\n",
      "store num 721\n",
      "store num 722\n",
      "store num 723\n",
      "store num 724\n",
      "store num 725\n",
      "store num 727\n",
      "store num 728\n",
      "store num 729\n",
      "store num 731\n",
      "store num 732\n",
      "store num 733\n",
      "store num 734\n",
      "store num 736\n",
      "store num 737\n",
      "store num 738\n",
      "store num 739\n",
      "store num 740\n",
      "store num 741\n",
      "store num 742\n",
      "store num 744\n",
      "store num 746\n",
      "store num 748\n",
      "store num 749\n",
      "store num 750\n",
      "store num 751\n",
      "store num 752\n",
      "store num 753\n",
      "store num 756\n",
      "store num 757\n",
      "store num 758\n",
      "store num 759\n",
      "store num 762\n",
      "store num 763\n",
      "store num 764\n",
      "store num 765\n",
      "store num 766\n",
      "store num 767\n",
      "store num 768\n",
      "store num 769\n",
      "store num 770\n",
      "store num 771\n",
      "store num 772\n",
      "store num 773\n",
      "store num 774\n",
      "store num 775\n",
      "store num 776\n",
      "store num 777\n",
      "store num 778\n",
      "store num 782\n",
      "store num 784\n",
      "store num 785\n",
      "store num 789\n",
      "store num 790\n",
      "store num 791\n",
      "store num 792\n",
      "store num 793\n",
      "store num 795\n",
      "store num 796\n",
      "store num 797\n",
      "store num 799\n",
      "store num 800\n",
      "store num 801\n",
      "store num 802\n",
      "store num 803\n",
      "store num 804\n",
      "store num 805\n",
      "store num 806\n",
      "store num 807\n",
      "store num 809\n",
      "store num 810\n",
      "store num 811\n",
      "store num 813\n",
      "store num 815\n",
      "store num 816\n",
      "store num 818\n",
      "store num 819\n",
      "store num 820\n",
      "store num 822\n",
      "store num 823\n",
      "store num 824\n",
      "store num 825\n",
      "store num 826\n",
      "store num 829\n",
      "store num 831\n",
      "store num 832\n",
      "store num 833\n",
      "store num 835\n",
      "store num 837\n",
      "store num 840\n",
      "store num 842\n",
      "store num 844\n",
      "store num 845\n",
      "store num 846\n",
      "store num 847\n",
      "store num 848\n",
      "store num 849\n",
      "store num 850\n",
      "store num 851\n",
      "store num 852\n",
      "store num 853\n",
      "store num 855\n",
      "store num 856\n",
      "store num 857\n",
      "store num 858\n",
      "store num 859\n",
      "store num 860\n",
      "store num 861\n",
      "store num 862\n",
      "store num 863\n",
      "store num 864\n",
      "store num 865\n",
      "store num 866\n",
      "store num 867\n",
      "store num 868\n",
      "store num 871\n",
      "store num 872\n",
      "store num 874\n",
      "store num 875\n",
      "store num 877\n",
      "store num 879\n",
      "store num 880\n",
      "store num 881\n",
      "store num 882\n",
      "store num 883\n",
      "store num 884\n",
      "store num 885\n",
      "store num 886\n",
      "store num 887\n",
      "store num 888\n",
      "store num 890\n",
      "store num 891\n",
      "store num 893\n",
      "store num 894\n",
      "store num 895\n",
      "store num 896\n",
      "store num 897\n",
      "store num 900\n",
      "store num 901\n",
      "store num 902\n",
      "store num 903\n",
      "store num 904\n",
      "store num 905\n",
      "store num 906\n",
      "store num 907\n",
      "store num 908\n",
      "store num 909\n",
      "store num 911\n",
      "store num 912\n",
      "store num 913\n",
      "store num 914\n",
      "store num 915\n",
      "store num 916\n",
      "store num 917\n",
      "store num 919\n",
      "store num 920\n",
      "store num 922\n",
      "store num 924\n",
      "store num 925\n",
      "store num 926\n",
      "store num 927\n",
      "store num 928\n",
      "store num 929\n",
      "store num 930\n",
      "store num 931\n",
      "store num 932\n",
      "store num 934\n",
      "store num 935\n",
      "store num 936\n",
      "store num 937\n",
      "store num 938\n",
      "store num 939\n",
      "store num 941\n",
      "store num 942\n",
      "store num 943\n",
      "store num 944\n",
      "store num 945\n",
      "store num 946\n",
      "store num 947\n",
      "store num 948\n",
      "store num 950\n",
      "store num 951\n",
      "store num 952\n",
      "store num 954\n",
      "store num 955\n",
      "store num 956\n",
      "store num 960\n",
      "store num 961\n",
      "store num 962\n",
      "store num 964\n",
      "store num 965\n",
      "store num 966\n",
      "store num 967\n",
      "store num 969\n",
      "store num 970\n",
      "store num 973\n",
      "store num 974\n",
      "store num 975\n",
      "store num 976\n",
      "store num 977\n",
      "store num 980\n",
      "store num 983\n",
      "store num 984\n",
      "store num 985\n",
      "store num 986\n",
      "store num 988\n",
      "store num 989\n",
      "store num 991\n",
      "store num 992\n",
      "store num 994\n",
      "store num 997\n",
      "store num 998\n",
      "store num 1000\n",
      "store num 1003\n",
      "store num 1004\n",
      "store num 1005\n",
      "store num 1007\n",
      "store num 1008\n",
      "store num 1009\n",
      "store num 1010\n",
      "store num 1011\n",
      "store num 1012\n",
      "store num 1013\n",
      "store num 1014\n",
      "store num 1015\n",
      "store num 1016\n",
      "store num 1019\n",
      "store num 1020\n",
      "store num 1022\n",
      "store num 1024\n",
      "store num 1025\n",
      "store num 1026\n",
      "store num 1027\n",
      "store num 1028\n",
      "store num 1031\n",
      "store num 1036\n",
      "store num 1037\n",
      "store num 1038\n",
      "store num 1039\n",
      "store num 1040\n",
      "store num 1041\n",
      "store num 1042\n",
      "store num 1044\n",
      "store num 1045\n",
      "store num 1047\n",
      "store num 1048\n",
      "store num 1049\n",
      "store num 1050\n",
      "store num 1051\n",
      "store num 1052\n",
      "store num 1053\n",
      "store num 1054\n",
      "store num 1056\n",
      "store num 1057\n",
      "store num 1058\n",
      "store num 1059\n",
      "store num 1060\n",
      "store num 1061\n",
      "store num 1062\n",
      "store num 1063\n",
      "store num 1064\n",
      "store num 1065\n",
      "store num 1066\n",
      "store num 1067\n",
      "store num 1068\n",
      "store num 1070\n",
      "store num 1071\n",
      "store num 1072\n",
      "store num 1073\n",
      "store num 1076\n",
      "store num 1077\n",
      "store num 1078\n",
      "store num 1079\n",
      "store num 1080\n",
      "store num 1083\n",
      "store num 1084\n",
      "store num 1086\n",
      "store num 1087\n",
      "store num 1088\n",
      "store num 1089\n",
      "store num 1091\n",
      "store num 1092\n",
      "store num 1094\n",
      "store num 1096\n",
      "store num 1097\n",
      "store num 1099\n",
      "store num 1100\n",
      "store num 1101\n",
      "store num 1102\n",
      "store num 1103\n",
      "store num 1104\n",
      "store num 1105\n",
      "store num 1106\n",
      "store num 1107\n",
      "store num 1109\n",
      "store num 1111\n",
      "store num 1112\n",
      "store num 1113\n",
      "store num 1114\n",
      "store num 1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libfun/.virtualenvs/main/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/libfun/.virtualenvs/main/lib/python2.7/site-packages/ipykernel/__main__.py:7: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "for store in stores:\n",
    "    print 'store num', store\n",
    "    train_x = train[train['Store'] == store]\n",
    "    train_x = train_x.sort(['Date'])\n",
    "    train_x.index = np.arange(len(train_x.index))\n",
    "    test_x = test[test['Store'] == store]\n",
    "    test_x = test_x.sort(['Date'])\n",
    "    test_x.index = np.arange(len(test_x.index))\n",
    "    train_7 = train_x.copy()\n",
    "    train_18 = train_x.copy()\n",
    "    train_30 = train_x.copy()\n",
    "    test_7 = test_x.copy()\n",
    "    test_18 = test_x.copy()\n",
    "    test_30 = test_x.copy()\n",
    "    for f in [i+'mean' for i in ts_features] + [i+'std' for i in ts_features] + [i+'dta' for i in ts_features]:\n",
    "        train_7.loc[:,f] = range(len(train_7))\n",
    "        test_7.loc[:,f] = range(len(test_7))\n",
    "        train_18.loc[:,f] = range(len(train_18))\n",
    "        test_18.loc[:,f] = range(len(test_18))\n",
    "        train_30.loc[:,f] = range(len(train_30))\n",
    "        test_30.loc[:,f] = range(len(test_30))\n",
    "    for step in xrange(6, len(train_7)):\n",
    "        s = max(step-6,0)\n",
    "        st = max(step-1, 0)\n",
    "        train_7.loc[step,[i+'mean' for i in ts_features]] = (train_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "        train_7.loc[step,[i+'std' for i in ts_features]] = (train_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "        train_7.loc[step,[i+'dta' for i in ts_features]] = (train_x.loc[st,ts_features] - \\\n",
    "                                                            train_x.loc[s,ts_features]).values\n",
    "    train_7 = train_7.loc[6:,:]\n",
    "    for step in xrange(len(test_7)):\n",
    "        s = max(step-6,0)\n",
    "        st = max(step-1, 0)\n",
    "        test_7.loc[step,[i+'mean' for i in ts_features]] = (test_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "        test_7.loc[step,[i+'std' for i in ts_features]] = (test_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "        test_7.loc[step,[i+'dta' for i in ts_features]] = (test_x.loc[st,ts_features] - \\\n",
    "                                                           train_x.loc[s,ts_features]).values\n",
    "    test_7 = test_7.fillna(0)\n",
    "    \n",
    "    for step in xrange(17, len(train_18)):\n",
    "        s = max(step-17,0)\n",
    "        st = max(step-1, 0)\n",
    "        train_18.loc[step,[i+'mean' for i in ts_features]] = (train_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "        train_18.loc[step,[i+'std' for i in ts_features]] = (train_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "        train_18.loc[step,[i+'dta' for i in ts_features]] = (train_x.loc[st,ts_features] - \\\n",
    "                                                             train_x.loc[s,ts_features]).values\n",
    "    train_18 = train_18.loc[17:,:]\n",
    "    for step in xrange(len(test_18)):\n",
    "        s = max(step-17,0)\n",
    "        st = max(step-1, 0)\n",
    "        test_18.loc[step,[i+'mean' for i in ts_features]] = (test_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "        test_18.loc[step,[i+'std' for i in ts_features]] = (test_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "        test_18.loc[step,[i+'dta' for i in ts_features]] = (test_x.loc[st,ts_features] - \\\n",
    "                                                            train_x.loc[s,ts_features]).values\n",
    "    test_18 = test_18.fillna(0)\n",
    "    \n",
    "    for step in xrange(29, len(train_30)):\n",
    "        s = max(step-29,0)\n",
    "        st = max(step-1, 0)\n",
    "        train_30.loc[step,[i+'mean' for i in ts_features]] = (train_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "        train_30.loc[step,[i+'std' for i in ts_features]] = (train_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "        train_30.loc[step,[i+'dta' for i in ts_features]] = (train_x.loc[st,ts_features] - \\\n",
    "                                                             train_x.loc[s,ts_features]).values\n",
    "    train_30 = train_30.loc[29:,:]\n",
    "    for step in xrange(len(test_30)):\n",
    "        s = max(step-29,0)\n",
    "        st = max(step-1, 0)\n",
    "        test_30.loc[step,[i+'mean' for i in ts_features]] = (test_x.loc[s:st,ts_features].mean(axis=0)).values\n",
    "        test_30.loc[step,[i+'std' for i in ts_features]] = (test_x.loc[s:st,ts_features].std(axis=0)).values\n",
    "        test_30.loc[step,[i+'dta' for i in ts_features]] = (test_x.loc[st,ts_features] - \\\n",
    "                                                            train_x.loc[s,ts_features]).values\n",
    "    test_30 = test_30.fillna(0)\n",
    "    \n",
    "    train_a7 = pd.concat([train_a7, train_7])\n",
    "    train_a18 = pd.concat([train_a18, train_18])\n",
    "    train_a30 = pd.concat([train_a30, train_30])\n",
    "    test_a7 = pd.concat([test_a7, test_7])\n",
    "    test_a18 = pd.concat([test_a18, test_18])\n",
    "    test_a30 = pd.concat([test_a30, test_30])\n",
    "    \n",
    "    #ans.loc[test_7.Id-1, 'Sales_7'] = np.exp(test_probs_7) - 1\n",
    "    #ans.loc[test_18.Id-1, 'Sales_18'] = np.exp(test_probs_18) - 1\n",
    "    #ans.loc[test_30.Id-1, 'Sales_30'] = np.exp(test_probs_30) - 1\n",
    "    #ans['Sales'] = ans.loc[:, ['Sales_7','Sales_18','Sales_30']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLOLO\n"
     ]
    }
   ],
   "source": [
    "print 'OLOLO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans[['Id', 'Sales']].to_csv(\"rf_ts_ind.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sales_7</th>\n",
       "      <th>Sales_18</th>\n",
       "      <th>Sales_30</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4251.382482</td>\n",
       "      <td>4168.469634</td>\n",
       "      <td>4103.432784</td>\n",
       "      <td>4174.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7642.082397</td>\n",
       "      <td>7560.451238</td>\n",
       "      <td>7827.801700</td>\n",
       "      <td>7676.778445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7735.843515</td>\n",
       "      <td>8044.769232</td>\n",
       "      <td>8473.068434</td>\n",
       "      <td>8084.560393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7137.878685</td>\n",
       "      <td>6824.343136</td>\n",
       "      <td>6937.614738</td>\n",
       "      <td>6966.612186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6141.323294</td>\n",
       "      <td>6192.697763</td>\n",
       "      <td>5655.505637</td>\n",
       "      <td>5996.508898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5440.689182</td>\n",
       "      <td>5710.045655</td>\n",
       "      <td>5792.587536</td>\n",
       "      <td>5647.774124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7228.827077</td>\n",
       "      <td>7413.990474</td>\n",
       "      <td>7950.286864</td>\n",
       "      <td>7531.034805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8191.153401</td>\n",
       "      <td>7715.232008</td>\n",
       "      <td>8211.151985</td>\n",
       "      <td>8039.179131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>5633.846755</td>\n",
       "      <td>5173.021793</td>\n",
       "      <td>5842.806794</td>\n",
       "      <td>5549.891781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5434.562313</td>\n",
       "      <td>5799.087453</td>\n",
       "      <td>5969.275602</td>\n",
       "      <td>5734.308456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>6354.865316</td>\n",
       "      <td>6818.246162</td>\n",
       "      <td>6697.133559</td>\n",
       "      <td>6623.415012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>7681.678153</td>\n",
       "      <td>7825.468022</td>\n",
       "      <td>7734.772019</td>\n",
       "      <td>7747.306065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>6827.062528</td>\n",
       "      <td>6908.461296</td>\n",
       "      <td>7041.769667</td>\n",
       "      <td>6925.764497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>9255.131508</td>\n",
       "      <td>8222.525284</td>\n",
       "      <td>7850.304600</td>\n",
       "      <td>8442.653797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>5838.447833</td>\n",
       "      <td>5524.306227</td>\n",
       "      <td>5751.737931</td>\n",
       "      <td>5704.830663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>4796.909953</td>\n",
       "      <td>4737.593746</td>\n",
       "      <td>4924.070372</td>\n",
       "      <td>4819.524690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5546.541887</td>\n",
       "      <td>5449.903732</td>\n",
       "      <td>5765.611898</td>\n",
       "      <td>5587.352506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>9202.351307</td>\n",
       "      <td>9687.710718</td>\n",
       "      <td>9737.214917</td>\n",
       "      <td>9542.425647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1659.047999</td>\n",
       "      <td>2538.785842</td>\n",
       "      <td>3302.214757</td>\n",
       "      <td>2500.016199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>9775.535151</td>\n",
       "      <td>10080.192420</td>\n",
       "      <td>10061.591870</td>\n",
       "      <td>9972.439814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>7078.581956</td>\n",
       "      <td>5749.109351</td>\n",
       "      <td>6285.165432</td>\n",
       "      <td>6370.952247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>4725.502266</td>\n",
       "      <td>4798.232397</td>\n",
       "      <td>4661.325923</td>\n",
       "      <td>4728.353529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>5336.175960</td>\n",
       "      <td>5599.632634</td>\n",
       "      <td>6089.176488</td>\n",
       "      <td>5674.995027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>4068.297944</td>\n",
       "      <td>4159.861714</td>\n",
       "      <td>4239.183937</td>\n",
       "      <td>4155.781198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>7455.806721</td>\n",
       "      <td>7575.193674</td>\n",
       "      <td>7560.138388</td>\n",
       "      <td>7530.379594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>9492.242559</td>\n",
       "      <td>10040.090868</td>\n",
       "      <td>9530.393690</td>\n",
       "      <td>9687.575705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>8628.876253</td>\n",
       "      <td>8851.927047</td>\n",
       "      <td>9022.909813</td>\n",
       "      <td>8834.571038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>5626.870285</td>\n",
       "      <td>5630.606206</td>\n",
       "      <td>5876.727987</td>\n",
       "      <td>5711.401493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>4881.049714</td>\n",
       "      <td>4425.164476</td>\n",
       "      <td>4515.550300</td>\n",
       "      <td>4607.254830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>4465.653630</td>\n",
       "      <td>4176.849800</td>\n",
       "      <td>4616.929517</td>\n",
       "      <td>4419.810982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41058</th>\n",
       "      <td>41059</td>\n",
       "      <td>2959.796046</td>\n",
       "      <td>2552.951560</td>\n",
       "      <td>2801.512554</td>\n",
       "      <td>2771.420053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41059</th>\n",
       "      <td>41060</td>\n",
       "      <td>5111.495760</td>\n",
       "      <td>4813.236876</td>\n",
       "      <td>5604.706449</td>\n",
       "      <td>5176.479695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41060</th>\n",
       "      <td>41061</td>\n",
       "      <td>6579.146547</td>\n",
       "      <td>6256.367874</td>\n",
       "      <td>6377.209539</td>\n",
       "      <td>6404.241320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41061</th>\n",
       "      <td>41062</td>\n",
       "      <td>10476.733049</td>\n",
       "      <td>10167.061830</td>\n",
       "      <td>10045.704051</td>\n",
       "      <td>10229.832977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41062</th>\n",
       "      <td>41063</td>\n",
       "      <td>6018.904035</td>\n",
       "      <td>5314.699502</td>\n",
       "      <td>5240.317830</td>\n",
       "      <td>5524.640456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41063</th>\n",
       "      <td>41064</td>\n",
       "      <td>5117.192049</td>\n",
       "      <td>4901.946548</td>\n",
       "      <td>5848.281935</td>\n",
       "      <td>5289.140177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41064</th>\n",
       "      <td>41065</td>\n",
       "      <td>3815.675474</td>\n",
       "      <td>3929.183169</td>\n",
       "      <td>4033.430887</td>\n",
       "      <td>3926.096510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41065</th>\n",
       "      <td>41066</td>\n",
       "      <td>5481.758335</td>\n",
       "      <td>5232.975763</td>\n",
       "      <td>6131.289637</td>\n",
       "      <td>5615.341245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41066</th>\n",
       "      <td>41067</td>\n",
       "      <td>4118.007899</td>\n",
       "      <td>3320.479010</td>\n",
       "      <td>3426.177669</td>\n",
       "      <td>3621.554859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41067</th>\n",
       "      <td>41068</td>\n",
       "      <td>8800.628922</td>\n",
       "      <td>8710.591076</td>\n",
       "      <td>10863.796254</td>\n",
       "      <td>9458.338751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41068</th>\n",
       "      <td>41069</td>\n",
       "      <td>6496.449158</td>\n",
       "      <td>6494.907387</td>\n",
       "      <td>6951.169954</td>\n",
       "      <td>6647.508833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41069</th>\n",
       "      <td>41070</td>\n",
       "      <td>14376.308800</td>\n",
       "      <td>12887.488282</td>\n",
       "      <td>13227.819624</td>\n",
       "      <td>13497.205569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41070</th>\n",
       "      <td>41071</td>\n",
       "      <td>5694.021280</td>\n",
       "      <td>4733.426782</td>\n",
       "      <td>5606.613885</td>\n",
       "      <td>5344.687316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41071</th>\n",
       "      <td>41072</td>\n",
       "      <td>4837.372999</td>\n",
       "      <td>3928.968811</td>\n",
       "      <td>4546.438498</td>\n",
       "      <td>4437.593436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41072</th>\n",
       "      <td>41073</td>\n",
       "      <td>7255.322992</td>\n",
       "      <td>7731.502459</td>\n",
       "      <td>8054.368031</td>\n",
       "      <td>7680.397827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41073</th>\n",
       "      <td>41074</td>\n",
       "      <td>6132.921475</td>\n",
       "      <td>5264.930567</td>\n",
       "      <td>6636.171876</td>\n",
       "      <td>6011.341306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41074</th>\n",
       "      <td>41075</td>\n",
       "      <td>399.638614</td>\n",
       "      <td>1565.722185</td>\n",
       "      <td>3352.147475</td>\n",
       "      <td>1772.502758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41075</th>\n",
       "      <td>41076</td>\n",
       "      <td>8392.548188</td>\n",
       "      <td>7822.473819</td>\n",
       "      <td>8101.888989</td>\n",
       "      <td>8105.636999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41076</th>\n",
       "      <td>41077</td>\n",
       "      <td>6443.519380</td>\n",
       "      <td>6222.768925</td>\n",
       "      <td>7044.182012</td>\n",
       "      <td>6570.156772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41077</th>\n",
       "      <td>41078</td>\n",
       "      <td>5351.619880</td>\n",
       "      <td>4698.855055</td>\n",
       "      <td>4599.688005</td>\n",
       "      <td>4883.387647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41078</th>\n",
       "      <td>41079</td>\n",
       "      <td>5894.533742</td>\n",
       "      <td>5652.740695</td>\n",
       "      <td>6111.597886</td>\n",
       "      <td>5886.290774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41079</th>\n",
       "      <td>41080</td>\n",
       "      <td>3894.621513</td>\n",
       "      <td>3043.003559</td>\n",
       "      <td>3845.970771</td>\n",
       "      <td>3594.531948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41080</th>\n",
       "      <td>41081</td>\n",
       "      <td>3994.426748</td>\n",
       "      <td>3585.663062</td>\n",
       "      <td>3720.541642</td>\n",
       "      <td>3766.877151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41081</th>\n",
       "      <td>41082</td>\n",
       "      <td>4879.205684</td>\n",
       "      <td>5011.472847</td>\n",
       "      <td>5776.492531</td>\n",
       "      <td>5222.390354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41082</th>\n",
       "      <td>41083</td>\n",
       "      <td>3777.413427</td>\n",
       "      <td>3778.043295</td>\n",
       "      <td>3813.756761</td>\n",
       "      <td>3789.737828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41083</th>\n",
       "      <td>41084</td>\n",
       "      <td>3561.133465</td>\n",
       "      <td>3212.555635</td>\n",
       "      <td>3244.154515</td>\n",
       "      <td>3339.281205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41084</th>\n",
       "      <td>41085</td>\n",
       "      <td>7119.781317</td>\n",
       "      <td>7053.082303</td>\n",
       "      <td>8110.988983</td>\n",
       "      <td>7427.950868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41085</th>\n",
       "      <td>41086</td>\n",
       "      <td>5409.385499</td>\n",
       "      <td>5250.050528</td>\n",
       "      <td>5527.291184</td>\n",
       "      <td>5395.575737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41086</th>\n",
       "      <td>41087</td>\n",
       "      <td>21639.626412</td>\n",
       "      <td>20542.810228</td>\n",
       "      <td>23558.466892</td>\n",
       "      <td>21913.634511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41087</th>\n",
       "      <td>41088</td>\n",
       "      <td>6688.863790</td>\n",
       "      <td>6272.058804</td>\n",
       "      <td>6217.393473</td>\n",
       "      <td>6392.772022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41088 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id       Sales_7      Sales_18      Sales_30         Sales\n",
       "0          1   4251.382482   4168.469634   4103.432784   4174.428300\n",
       "1          2   7642.082397   7560.451238   7827.801700   7676.778445\n",
       "2          3   7735.843515   8044.769232   8473.068434   8084.560393\n",
       "3          4   7137.878685   6824.343136   6937.614738   6966.612186\n",
       "4          5   6141.323294   6192.697763   5655.505637   5996.508898\n",
       "5          6   5440.689182   5710.045655   5792.587536   5647.774124\n",
       "6          7   7228.827077   7413.990474   7950.286864   7531.034805\n",
       "7          8   8191.153401   7715.232008   8211.151985   8039.179131\n",
       "8          9   5633.846755   5173.021793   5842.806794   5549.891781\n",
       "9         10   5434.562313   5799.087453   5969.275602   5734.308456\n",
       "10        11   6354.865316   6818.246162   6697.133559   6623.415012\n",
       "11        12   7681.678153   7825.468022   7734.772019   7747.306065\n",
       "12        13   6827.062528   6908.461296   7041.769667   6925.764497\n",
       "13        14   9255.131508   8222.525284   7850.304600   8442.653797\n",
       "14        15   5838.447833   5524.306227   5751.737931   5704.830663\n",
       "15        16   4796.909953   4737.593746   4924.070372   4819.524690\n",
       "16        17   5546.541887   5449.903732   5765.611898   5587.352506\n",
       "17        18   9202.351307   9687.710718   9737.214917   9542.425647\n",
       "18        19   1659.047999   2538.785842   3302.214757   2500.016199\n",
       "19        20   9775.535151  10080.192420  10061.591870   9972.439814\n",
       "20        21   7078.581956   5749.109351   6285.165432   6370.952247\n",
       "21        22   4725.502266   4798.232397   4661.325923   4728.353529\n",
       "22        23   5336.175960   5599.632634   6089.176488   5674.995027\n",
       "23        24   4068.297944   4159.861714   4239.183937   4155.781198\n",
       "24        25   7455.806721   7575.193674   7560.138388   7530.379594\n",
       "25        26   9492.242559  10040.090868   9530.393690   9687.575705\n",
       "26        27   8628.876253   8851.927047   9022.909813   8834.571038\n",
       "27        28   5626.870285   5630.606206   5876.727987   5711.401493\n",
       "28        29   4881.049714   4425.164476   4515.550300   4607.254830\n",
       "29        30   4465.653630   4176.849800   4616.929517   4419.810982\n",
       "...      ...           ...           ...           ...           ...\n",
       "41058  41059   2959.796046   2552.951560   2801.512554   2771.420053\n",
       "41059  41060   5111.495760   4813.236876   5604.706449   5176.479695\n",
       "41060  41061   6579.146547   6256.367874   6377.209539   6404.241320\n",
       "41061  41062  10476.733049  10167.061830  10045.704051  10229.832977\n",
       "41062  41063   6018.904035   5314.699502   5240.317830   5524.640456\n",
       "41063  41064   5117.192049   4901.946548   5848.281935   5289.140177\n",
       "41064  41065   3815.675474   3929.183169   4033.430887   3926.096510\n",
       "41065  41066   5481.758335   5232.975763   6131.289637   5615.341245\n",
       "41066  41067   4118.007899   3320.479010   3426.177669   3621.554859\n",
       "41067  41068   8800.628922   8710.591076  10863.796254   9458.338751\n",
       "41068  41069   6496.449158   6494.907387   6951.169954   6647.508833\n",
       "41069  41070  14376.308800  12887.488282  13227.819624  13497.205569\n",
       "41070  41071   5694.021280   4733.426782   5606.613885   5344.687316\n",
       "41071  41072   4837.372999   3928.968811   4546.438498   4437.593436\n",
       "41072  41073   7255.322992   7731.502459   8054.368031   7680.397827\n",
       "41073  41074   6132.921475   5264.930567   6636.171876   6011.341306\n",
       "41074  41075    399.638614   1565.722185   3352.147475   1772.502758\n",
       "41075  41076   8392.548188   7822.473819   8101.888989   8105.636999\n",
       "41076  41077   6443.519380   6222.768925   7044.182012   6570.156772\n",
       "41077  41078   5351.619880   4698.855055   4599.688005   4883.387647\n",
       "41078  41079   5894.533742   5652.740695   6111.597886   5886.290774\n",
       "41079  41080   3894.621513   3043.003559   3845.970771   3594.531948\n",
       "41080  41081   3994.426748   3585.663062   3720.541642   3766.877151\n",
       "41081  41082   4879.205684   5011.472847   5776.492531   5222.390354\n",
       "41082  41083   3777.413427   3778.043295   3813.756761   3789.737828\n",
       "41083  41084   3561.133465   3212.555635   3244.154515   3339.281205\n",
       "41084  41085   7119.781317   7053.082303   8110.988983   7427.950868\n",
       "41085  41086   5409.385499   5250.050528   5527.291184   5395.575737\n",
       "41086  41087  21639.626412  20542.810228  23558.466892  21913.634511\n",
       "41087  41088   6688.863790   6272.058804   6217.393473   6392.772022\n",
       "\n",
       "[41088 rows x 5 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_a7.to_csv('../train_a7.csv', index = False)\n",
    "train_a18.to_csv('../train_a18.csv', index = False)\n",
    "train_a30.to_csv('../train_a30.csv', index = False)\n",
    "test_a7.to_csv('../test_a7.csv', index = False)\n",
    "test_a18.to_csv('../test_a18.csv', index = False)\n",
    "test_a30.to_csv('../test_a30.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = cross_validation.train_test_split(train_a7, test_size=0.05, random_state = 1)\n",
    "dtrain = xgb.DMatrix(X_train[X_train['Open'] > 0][features], np.log(X_train[X_train['Open'] > 0][\"Sales\"] + 1))\n",
    "dvalid = xgb.DMatrix(X_test[X_test['Open'] > 0][features], np.log(X_test[X_test['Open'] > 0][\"Sales\"] + 1))\n",
    "\n",
    "def calc(params):\n",
    "\n",
    "    print params\n",
    "    \n",
    "    num_trees = 5000\n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, \n",
    "                    feval=rmspe_xg, verbose_eval=False)\n",
    "\n",
    "    #print(\"Validating\")\n",
    "    train_probs = gbm.predict(dvalid)\n",
    "    indices = train_probs < 0\n",
    "    train_probs[indices] = 0\n",
    "    error = rmspe(np.exp(train_probs) - 1, X_test[X_test['Open'] > 0]['Sales'].values)\n",
    "    print 'Error:', error\n",
    "    return {'loss': error, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[868]\ttrain-rmspe:0.131505\teval-rmspe:0.128933\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.55, 'silent': 1, 'subsample': 0.75, 'eta': 0.375, 'objective': 'reg:linear', 'alpha': 1.0, 'seed': 1, 'max_depth': 6.0, 'gamma': 0.8, 'lambda': 227.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[809]\ttrain-rmspe:0.137744\teval-rmspe:0.135074\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.128939796017\n",
      "{'colsample_bytree': 0.4, 'silent': 1, 'subsample': 0.30000000000000004, 'eta': 0.2, 'objective': 'reg:linear', 'alpha': 0.65, 'seed': 1, 'max_depth': 15.0, 'gamma': 0.75, 'lambda': 572.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1464]\ttrain-rmspe:0.154927\teval-rmspe:0.153385\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.135115517404\n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'subsample': 0.6000000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'alpha': 0.5, 'seed': 1, 'max_depth': 2.0, 'gamma': 0.55, 'lambda': 628.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1509]\ttrain-rmspe:0.141354\teval-rmspe:0.139361\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.153397406643\n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'subsample': 0.30000000000000004, 'eta': 0.375, 'objective': 'reg:linear', 'alpha': 0.6000000000000001, 'seed': 1, 'max_depth': 3.0, 'gamma': 0.55, 'lambda': 87.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[809]\ttrain-rmspe:0.154100\teval-rmspe:0.153010\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.139381347978\n",
      "{'colsample_bytree': 0.25, 'silent': 1, 'subsample': 0.6000000000000001, 'eta': 0.275, 'objective': 'reg:linear', 'alpha': 0.45, 'seed': 1, 'max_depth': 2.0, 'gamma': 0.9, 'lambda': 186.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3734]\ttrain-rmspe:0.126153\teval-rmspe:0.122904\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.153084495616\n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'subsample': 0.9, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'alpha': 0.8, 'seed': 1, 'max_depth': 6.0, 'gamma': 0.5, 'lambda': 131.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1495]\ttrain-rmspe:0.130124\teval-rmspe:0.127158\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.122906051432\n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'subsample': 0.65, 'eta': 0.47500000000000003, 'objective': 'reg:linear', 'alpha': 0.9, 'seed': 1, 'max_depth': 6.0, 'gamma': 0.55, 'lambda': 605.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1005]\ttrain-rmspe:0.135955\teval-rmspe:0.133234\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.127172632479\n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'subsample': 0.30000000000000004, 'eta': 0.25, 'objective': 'reg:linear', 'alpha': 0.4, 'seed': 1, 'max_depth': 7.0, 'gamma': 0.55, 'lambda': 555.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1039]\ttrain-rmspe:0.128798\teval-rmspe:0.125033\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.13324601079\n",
      "{'colsample_bytree': 0.30000000000000004, 'silent': 1, 'subsample': 0.7000000000000001, 'eta': 0.35000000000000003, 'objective': 'reg:linear', 'alpha': 0.25, 'seed': 1, 'max_depth': 19.0, 'gamma': 0.8, 'lambda': 388.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1136]\ttrain-rmspe:0.131955\teval-rmspe:0.128047\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.125043142755\n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'subsample': 0.8, 'eta': 0.4, 'objective': 'reg:linear', 'alpha': 0.9500000000000001, 'seed': 1, 'max_depth': 8.0, 'gamma': 0.9, 'lambda': 829.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1462]\ttrain-rmspe:0.140661\teval-rmspe:0.138462\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.128063723963\n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'subsample': 0.55, 'eta': 0.225, 'objective': 'reg:linear', 'alpha': 0.2, 'seed': 1, 'max_depth': 3.0, 'gamma': 0.55, 'lambda': 110.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[965]\ttrain-rmspe:0.130160\teval-rmspe:0.127122\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.138500967696\n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'subsample': 0.8500000000000001, 'eta': 0.35000000000000003, 'objective': 'reg:linear', 'alpha': 0.7000000000000001, 'seed': 1, 'max_depth': 10.0, 'gamma': 0.9500000000000001, 'lambda': 628.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1609]\ttrain-rmspe:0.130429\teval-rmspe:0.128183\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.127156083929\n",
      "{'colsample_bytree': 0.35000000000000003, 'silent': 1, 'subsample': 0.75, 'eta': 0.4, 'objective': 'reg:linear', 'alpha': 0.5, 'seed': 1, 'max_depth': 7.0, 'gamma': 0.9500000000000001, 'lambda': 195.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1944]\ttrain-rmspe:0.139983\teval-rmspe:0.137067\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.128193139882\n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'subsample': 0.30000000000000004, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'alpha': 0.35000000000000003, 'seed': 1, 'max_depth': 4.0, 'gamma': 0.6000000000000001, 'lambda': 213.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1288]\ttrain-rmspe:0.143906\teval-rmspe:0.141298\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.137098601009\n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'subsample': 0.9500000000000001, 'eta': 0.25, 'objective': 'reg:linear', 'alpha': 0.9, 'seed': 1, 'max_depth': 3.0, 'gamma': 0.8500000000000001, 'lambda': 695.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[792]\ttrain-rmspe:0.129990\teval-rmspe:0.126349\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.141306080866\n",
      "{'colsample_bytree': 0.25, 'silent': 1, 'subsample': 0.8, 'eta': 0.42500000000000004, 'objective': 'reg:linear', 'alpha': 0.4, 'seed': 1, 'max_depth': 14.0, 'gamma': 1.0, 'lambda': 324.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2804]\ttrain-rmspe:0.134522\teval-rmspe:0.131325\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.126357321189\n",
      "{'colsample_bytree': 0.35000000000000003, 'silent': 1, 'subsample': 0.6000000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'alpha': 0.30000000000000004, 'seed': 1, 'max_depth': 15.0, 'gamma': 1.0, 'lambda': 934.0}\n",
      "Error: 0.131344011833\n",
      "{'colsample_bytree': 0.45, 'silent': 1, 'subsample': 0.8, 'eta': 0.025, 'objective': 'reg:linear', 'alpha': 0.1, 'seed': 1, 'max_depth': 3.0, 'gamma': 0.65, 'lambda': 1.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Will train until eval error hasn't decreased in 50 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.14318877457\n",
      "{'colsample_bytree': 0.30000000000000004, 'silent': 1, 'subsample': 0.9500000000000001, 'eta': 0.375, 'objective': 'reg:linear', 'alpha': 0.05, 'seed': 1, 'max_depth': 11.0, 'gamma': 0.7000000000000001, 'lambda': 43.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[1089]\ttrain-rmspe:0.122730\teval-rmspe:0.119351\n",
      "\n",
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2392]\ttrain-rmspe:0.130338\teval-rmspe:0.126807\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.119382071016\n",
      "{'colsample_bytree': 0.30000000000000004, 'silent': 1, 'subsample': 0.9, 'eta': 0.125, 'objective': 'reg:linear', 'alpha': 0.75, 'seed': 1, 'max_depth': 12.0, 'gamma': 0.9, 'lambda': 656.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[559]\ttrain-rmspe:0.115078\teval-rmspe:0.118303\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.126811003646\n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'subsample': 1.0, 'eta': 0.30000000000000004, 'objective': 'reg:linear', 'alpha': 0.8, 'seed': 1, 'max_depth': 11.0, 'gamma': 0.65, 'lambda': 1.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[575]\ttrain-rmspe:0.113384\teval-rmspe:0.114429\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.118312750539\n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'subsample': 1.0, 'eta': 0.5, 'objective': 'reg:linear', 'alpha': 0.1, 'seed': 1, 'max_depth': 18.0, 'gamma': 0.7000000000000001, 'lambda': 17.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[964]\ttrain-rmspe:0.129538\teval-rmspe:0.125826\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.114434593747\n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'subsample': 0.45, 'eta': 0.5, 'objective': 'reg:linear', 'alpha': 0.15000000000000002, 'seed': 1, 'max_depth': 20.0, 'gamma': 0.7000000000000001, 'lambda': 420.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1089]\ttrain-rmspe:0.128297\teval-rmspe:0.125695\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.125847275725\n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'subsample': 0.45, 'eta': 0.30000000000000004, 'objective': 'reg:linear', 'alpha': 0.8, 'seed': 1, 'max_depth': 18.0, 'gamma': 0.65, 'lambda': 288.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[40]\ttrain-rmspe:0.115283\teval-rmspe:0.122538\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.125748129646\n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'subsample': 1.0, 'eta': 0.45, 'objective': 'reg:linear', 'alpha': 0.6000000000000001, 'seed': 1, 'max_depth': 17.0, 'gamma': 0.65, 'lambda': 4.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[354]\ttrain-rmspe:0.126108\teval-rmspe:0.122690\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.12253802901\n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'subsample': 1.0, 'eta': 0.5, 'objective': 'reg:linear', 'alpha': 0.55, 'seed': 1, 'max_depth': 13.0, 'gamma': 0.75, 'lambda': 408.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[549]\ttrain-rmspe:0.126551\teval-rmspe:0.123031\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.122692619703\n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'subsample': 1.0, 'eta': 0.30000000000000004, 'objective': 'reg:linear', 'alpha': 0.05, 'seed': 1, 'max_depth': 17.0, 'gamma': 0.7000000000000001, 'lambda': 774.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1741]\ttrain-rmspe:0.120915\teval-rmspe:0.118826\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.123033344146\n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'subsample': 0.9, 'eta': 0.2, 'objective': 'reg:linear', 'alpha': 0.8500000000000001, 'seed': 1, 'max_depth': 9.0, 'gamma': 0.6000000000000001, 'lambda': 47.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1215]\ttrain-rmspe:0.127225\teval-rmspe:0.123993\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.118833152653\n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'subsample': 0.5, 'eta': 0.325, 'objective': 'reg:linear', 'alpha': 0.0, 'seed': 1, 'max_depth': 16.0, 'gamma': 0.75, 'lambda': 280.0}\n",
      "Error: 0.124034427276\n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'subsample': 0.7000000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'alpha': 0.75, 'seed': 1, 'max_depth': 20.0, 'gamma': 0.8, 'lambda': 508.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Will train until eval error hasn't decreased in 50 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.125478055275\n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'subsample': 0.9500000000000001, 'eta': 0.45, 'objective': 'reg:linear', 'alpha': 1.0, 'seed': 1, 'max_depth': 12.0, 'gamma': 0.6000000000000001, 'lambda': 985.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[1617]\ttrain-rmspe:0.126337\teval-rmspe:0.123657\n",
      "\n",
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1184]\ttrain-rmspe:0.126921\teval-rmspe:0.124554\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.123666983083\n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'subsample': 0.4, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'alpha': 0.65, 'seed': 1, 'max_depth': 14.0, 'gamma': 0.7000000000000001, 'lambda': 131.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2380]\ttrain-rmspe:0.121702\teval-rmspe:0.119483\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.124572999476\n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'subsample': 0.8500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'alpha': 1.0, 'seed': 1, 'max_depth': 10.0, 'gamma': 0.8, 'lambda': 9.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[877]\ttrain-rmspe:0.121289\teval-rmspe:0.118433\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.119492476521\n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'subsample': 0.8500000000000001, 'eta': 0.45, 'objective': 'reg:linear', 'alpha': 0.55, 'seed': 1, 'max_depth': 18.0, 'gamma': 0.5, 'lambda': 350.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1165]\ttrain-rmspe:0.122970\teval-rmspe:0.120592\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.1184440733\n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'subsample': 0.75, 'eta': 0.275, 'objective': 'reg:linear', 'alpha': 0.25, 'seed': 1, 'max_depth': 15.0, 'gamma': 0.65, 'lambda': 253.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1607]\ttrain-rmspe:0.141352\teval-rmspe:0.139000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.120599203616\n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'subsample': 0.25, 'eta': 0.2, 'objective': 'reg:linear', 'alpha': 0.65, 'seed': 1, 'max_depth': 5.0, 'gamma': 0.75, 'lambda': 465.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1136]\ttrain-rmspe:0.124031\teval-rmspe:0.121187\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.139058750838\n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'subsample': 0.65, 'eta': 0.47500000000000003, 'objective': 'reg:linear', 'alpha': 0.4, 'seed': 1, 'max_depth': 9.0, 'gamma': 0.6000000000000001, 'lambda': 156.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[798]\ttrain-rmspe:0.170980\teval-rmspe:0.168142\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.121249116053\n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'subsample': 0.9500000000000001, 'eta': 0.325, 'objective': 'reg:linear', 'alpha': 0.45, 'seed': 1, 'max_depth': 1.0, 'gamma': 0.8500000000000001, 'lambda': 60.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1495]\ttrain-rmspe:0.119921\teval-rmspe:0.117466\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.168161060194\n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'subsample': 0.9, 'eta': 0.4, 'objective': 'reg:linear', 'alpha': 0.9, 'seed': 1, 'max_depth': 12.0, 'gamma': 0.5, 'lambda': 176.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[690]\ttrain-rmspe:0.117304\teval-rmspe:0.116643\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.117471663771\n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'subsample': 0.7000000000000001, 'eta': 0.4, 'objective': 'reg:linear', 'alpha': 0.2, 'seed': 1, 'max_depth': 19.0, 'gamma': 0.5, 'lambda': 95.0}\n",
      "Error:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[433]\ttrain-rmspe:0.128076\teval-rmspe:0.125415\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.116654832094\n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'subsample': 0.35000000000000003, 'eta': 0.47500000000000003, 'objective': 'reg:linear', 'alpha': 0.15000000000000002, 'seed': 1, 'max_depth': 19.0, 'gamma': 0.8500000000000001, 'lambda': 104.0}\n",
      "Error: 0.125535629917\n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'subsample': 0.55, 'eta': 0.42500000000000004, 'objective': 'reg:linear', 'alpha': 0.0, 'seed': 1, 'max_depth': 19.0, 'gamma': 0.55, 'lambda': 240.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 50 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c759ba0b8c3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#trials = Trials()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-c759ba0b8c3c>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m              }\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rseed)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFMinIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    112\u001b[0m             pyll_rval = pyll.rec_eval(self.expr, memo=memo,\n\u001b[0;32m    113\u001b[0m                     print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-e41aa5633e26>\u001b[0m in \u001b[0;36mcalc\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eval'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, \n\u001b[1;32m---> 13\u001b[1;33m                     feval=rmspe_xg, verbose_eval=False)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#print(\"Validating\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/xgboost-0.4-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, early_stopping_rounds, evals_result, verbose_eval, learning_rates)\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                     \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'eta'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m             \u001b[0mbst_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/libfun/.virtualenvs/main/local/lib/python2.7/site-packages/xgboost-0.4-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def score(params):\n",
    "    print \"Training with params : \"\n",
    "    print params\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid).reshape((X_test.shape[0], 9))\n",
    "    score = log_loss(y_test, predictions)\n",
    "    print \"\\tScore {0}\\n\\n\".format(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize():\n",
    "    space = {\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 20, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.2, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.2, 1, 0.05),\n",
    "             'alpha' : hp.quniform('alpha', 0, 1, 0.05),\n",
    "             'lambda' : hp.quniform('lambda', 0, 1000, 1),\n",
    "             'objective': 'reg:linear',\n",
    "             'silent' : 1,\n",
    "             'seed' : 1\n",
    "             }\n",
    "\n",
    "    best = fmin(calc, space, algo=tpe.suggest, max_evals=2500)\n",
    "\n",
    "    print best\n",
    "    \n",
    "    return best\n",
    "\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "#trials = Trials()\n",
    "\n",
    "best = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StateHoliday_0</th>\n",
       "      <th>...</th>\n",
       "      <th>StoreType_c</th>\n",
       "      <th>StoreType_d</th>\n",
       "      <th>Assortment_a</th>\n",
       "      <th>Assortment_b</th>\n",
       "      <th>Assortment_c</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>Mean_Sales</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3835.285714</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek       Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          2 2013-01-01      0          0     0      0            a   \n",
       "\n",
       "   SchoolHoliday  StateHoliday_0 ...   StoreType_c  StoreType_d  Assortment_a  \\\n",
       "0              1               0 ...             1            0             1   \n",
       "\n",
       "   Assortment_b  Assortment_c  CompetitionOpen   Mean_Sales  year  month  day  \n",
       "0             0             0                1  3835.285714  2013      1    1  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.loc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StateHoliday_0</th>\n",
       "      <th>...</th>\n",
       "      <th>StoreType_c</th>\n",
       "      <th>StoreType_d</th>\n",
       "      <th>Assortment_a</th>\n",
       "      <th>Assortment_b</th>\n",
       "      <th>Assortment_c</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>Mean_Sales</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3835.285714</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>5530</td>\n",
       "      <td>668</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3746.746032</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>4327</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3419.396825</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>4486</td>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4206.063492</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>4997</td>\n",
       "      <td>635</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4942.970149</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>7176</td>\n",
       "      <td>785</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5852.708333</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek       Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          2 2013-01-01      0          0     0      0            a   \n",
       "1      1          3 2013-01-02   5530        668     1      0            0   \n",
       "2      1          4 2013-01-03   4327        578     1      0            0   \n",
       "3      1          5 2013-01-04   4486        619     1      0            0   \n",
       "4      1          6 2013-01-05   4997        635     1      0            0   \n",
       "5      1          7 2013-01-06      0          0     0      0            0   \n",
       "6      1          1 2013-01-07   7176        785     1      1            0   \n",
       "\n",
       "   SchoolHoliday  StateHoliday_0 ...   StoreType_c  StoreType_d  Assortment_a  \\\n",
       "0              1               0 ...             1            0             1   \n",
       "1              1               1 ...             1            0             1   \n",
       "2              1               1 ...             1            0             1   \n",
       "3              1               1 ...             1            0             1   \n",
       "4              1               1 ...             1            0             1   \n",
       "5              1               1 ...             1            0             1   \n",
       "6              1               1 ...             1            0             1   \n",
       "\n",
       "   Assortment_b  Assortment_c  CompetitionOpen   Mean_Sales  year  month  day  \n",
       "0             0             0                1  3835.285714  2013      1    1  \n",
       "1             0             0                1  3746.746032  2013      1    2  \n",
       "2             0             0                1  3419.396825  2013      1    3  \n",
       "3             0             0                1  4206.063492  2013      1    4  \n",
       "4             0             0                1  4942.970149  2013      1    5  \n",
       "5             0             0                1     0.000000  2013      1    6  \n",
       "6             0             0                1  5852.708333  2013      1    7  \n",
       "\n",
       "[7 rows x 33 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.loc[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_z = train_x.loc[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_z.loc[0,:] = train_x.loc[0:6].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_z.loc[1,:] = train_x.loc[1:7].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                     1.000000\n",
       "DayOfWeek                 4.000000\n",
       "Sales                  4585.142857\n",
       "Customers               562.714286\n",
       "Open                      0.857143\n",
       "Promo                     0.285714\n",
       "SchoolHoliday             1.000000\n",
       "StateHoliday_0            1.000000\n",
       "StateHoliday_a            0.000000\n",
       "DayOfWeek_1               0.142857\n",
       "DayOfWeek_2               0.142857\n",
       "DayOfWeek_3               0.142857\n",
       "DayOfWeek_4               0.142857\n",
       "DayOfWeek_5               0.142857\n",
       "DayOfWeek_6               0.142857\n",
       "DayOfWeek_7               0.142857\n",
       "CompetitionDistance    1270.000000\n",
       "Promo2                    0.000000\n",
       "StoreType_a               0.000000\n",
       "StoreType_b               0.000000\n",
       "StoreType_c               1.000000\n",
       "StoreType_d               0.000000\n",
       "Assortment_a              1.000000\n",
       "Assortment_b              0.000000\n",
       "Assortment_c              0.000000\n",
       "CompetitionOpen           1.000000\n",
       "Mean_Sales             3933.211722\n",
       "year                   2013.000000\n",
       "month                     1.000000\n",
       "day                       5.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.loc[1:7].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StateHoliday_0</th>\n",
       "      <th>...</th>\n",
       "      <th>StoreType_c</th>\n",
       "      <th>StoreType_d</th>\n",
       "      <th>Assortment_a</th>\n",
       "      <th>Assortment_b</th>\n",
       "      <th>Assortment_c</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>Mean_Sales</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaT</td>\n",
       "      <td>3788.000000</td>\n",
       "      <td>469.285714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3697.517640</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4585.142857</td>\n",
       "      <td>562.714286</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3933.211722</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>4327.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3419.396825</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>4486.000000</td>\n",
       "      <td>619.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4206.063492</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>4997.000000</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4942.970149</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>7176.000000</td>\n",
       "      <td>785.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5852.708333</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek       Date        Sales   Customers      Open     Promo  \\\n",
       "0      1          4        NaT  3788.000000  469.285714  0.714286  0.142857   \n",
       "1      1          4        NaT  4585.142857  562.714286  0.857143  0.285714   \n",
       "2      1          4 2013-01-03  4327.000000  578.000000  1.000000  0.000000   \n",
       "3      1          5 2013-01-04  4486.000000  619.000000  1.000000  0.000000   \n",
       "4      1          6 2013-01-05  4997.000000  635.000000  1.000000  0.000000   \n",
       "5      1          7 2013-01-06     0.000000    0.000000  0.000000  0.000000   \n",
       "6      1          1 2013-01-07  7176.000000  785.000000  1.000000  1.000000   \n",
       "\n",
       "  StateHoliday  SchoolHoliday  StateHoliday_0    ...     StoreType_c  \\\n",
       "0          NaN              1        0.857143    ...               1   \n",
       "1          NaN              1        1.000000    ...               1   \n",
       "2            0              1        1.000000    ...               1   \n",
       "3            0              1        1.000000    ...               1   \n",
       "4            0              1        1.000000    ...               1   \n",
       "5            0              1        1.000000    ...               1   \n",
       "6            0              1        1.000000    ...               1   \n",
       "\n",
       "   StoreType_d  Assortment_a  Assortment_b  Assortment_c  CompetitionOpen  \\\n",
       "0            0             1             0             0                1   \n",
       "1            0             1             0             0                1   \n",
       "2            0             1             0             0                1   \n",
       "3            0             1             0             0                1   \n",
       "4            0             1             0             0                1   \n",
       "5            0             1             0             0                1   \n",
       "6            0             1             0             0                1   \n",
       "\n",
       "    Mean_Sales  year  month       day  \n",
       "0  3697.517640  2013      1  4.428571  \n",
       "1  3933.211722  2013      1  5.000000  \n",
       "2  3419.396825  2013      1  3.000000  \n",
       "3  4206.063492  2013      1  4.000000  \n",
       "4  4942.970149  2013      1  5.000000  \n",
       "5     0.000000  2013      1  6.000000  \n",
       "6  5852.708333  2013      1  7.000000  \n",
       "\n",
       "[7 rows x 33 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
